#+TITLE: ACM Tech Stack
#+DESCRIPTION: Moving all of ACM's services to cloud providers is going to be a tough one.

This repository includes all the files necessary to describe and deploy the
technology stack of ACM @ UCSD. You will find files such as:

- Ansible roles and playbooks
- Terraform files
- Custom patches and images for certain services

* Introduction

ACM @ UCSD's tech stack is complex enough to warrant documentation. This repository,
as well as this document, provide a description so as to test and deploy the technology
stack.

Deployment and provisioning of instances supported by Ansible and Terraform.

* Tech Stack Layout
The tech stack has the following main services:
- Membership Portal UI & API
- ACM Live
- Minecraft Server
- BreadBot
- Password Manager

Services will be deployed on separate instances of reasonable size for
each one.

We will begin by provisioning instances for each service.

* Provisioning
Speaking with Aaron Eason about the system specifications of his previous instances
for each service, we know that:
- ACM Live requires 16 GB of RAM and 4 vCPU cores.
- The Minecraft server requires 8 GB of RAM.

The other services most likely require a maximum of 1 GB of RAM.[fn:current-stack-requirements]

In order to maintain uptime for all components of the stack, each
service will run on a separate instance of appropriate size.

As of now, AWS takes too much of a cost for all of the services above; therefore,
we'll split the cost between Azure and AWS.

Additionally, each service will be provisioned with separate Ansible
playbooks.

We'll begin with the easiest instances to plan, which are those for
small services; BreadBot and Password Manager.

#+BEGIN_SRC terraform :tangle instances.tf
provider "aws" {
  region = "us-west-1"
}

# Ubuntu 20.04 LTS instance, for good measure
# Find other AMI's at https://cloud-images.ubuntu.com/locator/ec2/
resource "aws_instance" "breadbot" {
  ami           = "ami-0cd230f950c3de5d8"
  instance_type = "t3a.nano"
  tags {
    Name = "BreadBot"
  }
}

resource "aws_instance" "pass" {
  ami           = "ami-0cd230f950c3de5d8"
  instance_type = "t3a.nano"
  tags {
    Name = "Password Manager"
  }
}
#+END_SRC

ACM Live requires a more demanding server. Particularly, =t3= instances
shouldn't be used due to their potential CPU throttling on a server
with constant workload. We can use =m5= instances:

#+BEGIN_SRC terraform :tangle instances.tf
# ACM Live runs on Ubuntu 16.04 LTS
# We'll use a HVM instance of it, no extra stuff on top.
resource "aws_instance" "live" {
  ami           = "ami-0a63cd87767e10ed4"
  instance_type = "m5a.large"
  tags {
    Name = "ACM Live"
  }
}

#+END_SRC

Minecraft is too expensive to run on AWS; a third-party provider might be more
viable in terms of budget. For now, we'll exclude from AWS provisioning.

We will also include instances for the membership portal; we will need
one for the API, and we'll use AWS' RDS in order to host the PostgreSQL
database:

#+BEGIN_SRC terraform :tangle instances.tf
resource "aws_instance" "membership-portal" {
  ami           = "ami-0a63cd87767e10ed4"
  instance_type = "t3a.small"
  tags {
    Name = "Membership Portal API"
  }
}

resource "aws_db_instance" "membership-portal-db" {
  allocated_storage = 10
  engine            = "postgres"
  instance_class    = "db.t3.micro"
  name              = var.dbName
  username          = var.dbUser
  password          = var.dbPass
}
#+END_SRC

Note the above variables used originate using the provided =.env= file. Edit
its contents with values of your choice. The playbooks will also use them
to properly deploy =.env= files.

Using the [[https://calculator.aws][AWS Calculator]], we obtain the cost for all the instances per year:
| Instance            | Instance Type | Cost Per Month |
|---------------------+---------------+----------------|
| BreadBot            | t3a.nano      |           5.29 |
| Pass                | t3a.nano      |           5.29 |
| ACM Live            | m5a.large     |          85.73 |
| Portal API          | t3a.small     |          17.48 |
| Portal Database     | db.t3.micro   |          16.55 |
|---------------------+---------------+----------------|
| Total Cost / Month: |               |         130.34 |
| Total Cost / Year:  |               |        1564.08 |
|---------------------+---------------+----------------|
#+TBLFM: @7$3=vsum(@I..@II)::@8$3=vsum(@I..@II)*12

Assuming all goes well, running Terraform will deploy the stack.
#+BEGIN_SRC sh
terraform init && terraform apply
#+END_SRC

We will now begin deploying the software for each service.
We'll document the SSH commands used in tandem with the respective Ansible task
and role.

[fn:current-stack-requirements] Funnily enough, this is hard to quantify properly. Pass and BreadBot both occupied the GCP =f1.micro=, which has 0.6 GB of RAM, so maybe a bit more is useful. The API, however, is up for discussion, considering Heroku's less obvious nature with system requirements.

* Before Configuring
For debugging purposes, the =tech-stack= repo provides
a Vagrantfile to deploy test instances locally. You can boot each of them by
installing Vagrant and VirtualBox; afterwards, you run this command in the terminal:

#+BEGIN_SRC sh
vagrant up
#+END_SRC

For the purposes of making this documentation easier to follow, we'll assume
that you are provisioning instances to configure using either the provided
Terraform file or the provided Vagrantfile.

=tech-stack= provides two Ansible inventory files in order to ease its use;
=testing= and =production=; =testing= points to the local Ansible boxes, whereas
=production= points to the Terraform-provisioned instances; you may use either
one by including it in each Ansible playbook command:
#+BEGIN_SRC sh
ansible-playbook -i <inventory>
#+END_SRC

* Password Manager
:PROPERTIES:

:END:

The password manager is, in essence, a =bitwarden_rs= instance deployed on top
of a Bitwarden image. We will use the =pass= Ansible host for all the following
commands once we start building the playbook.

The easiest way, by far, to install =bitwarden_rs= is to use the Docker Compose tutorial
provided by the [[https://github.com/dani-garcia/bitwarden_rs/wiki/Using-Docker-Compose][bitwarden_rs wiki]].

First, configure the remote machine to be able to connect to it using your SSH
keys. Either [[https://www.ssh.com/ssh/keygen/][create a new one]] or provide the public key. If developing using a
Vagrant box, use this command to import the =pass= settings to your
configuration:

#+BEGIN_SRC sh
vagrant ssh-config --host pass >> $HOME/.ssh/config
#+END_SRC

You can also set these parameters manually in your SSH configuration file for
the Terraform-provisioned instances.[fn:ssh-terraform]

First, we'll want to install Docker by following the [[https://docs.docker.com/engine/install/ubuntu/][installation guide]] for Ubuntu:
#+BEGIN_SRC sh :dir /ssh:pass:~/ :results drawer
sudo apt-get update
sudo apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io
#+END_SRC

Similarly for Docker Compose, using this [[https://docs.docker.com/compose/install/][installation guide]]:

#+BEGIN_SRC sh :dir /ssh:pass:~/
sudo curl -L "https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
docker-compose --version
#+END_SRC

#+RESULTS:
| docker-compose version 1.26.2 | build eefe0d31 |

Second, we'll need to copy over the Docker Compose file for the =pass= instance, provided
in the repo, along with the environment variables for it:

#+BEGIN_SRC sh
scp ./files/pass/docker-compose.yml pass:~/
scp ./files/pass/.env pass:~/
scp ./files/pass/Caddyfile pass:~/
#+END_SRC

Afterwards, we'll want to log into the pass instance and create the directory used
by =bitwarden_rs= in the same directory.

#+BEGIN_SRC sh :dir /ssh:pass:~/
mkdir ~/bw-data
#+END_SRC

Afterwards, all we have to do is spin up the Docker Compose service.

#+BEGIN_SRC sh :dir /ssh:pass:~/
docker-compose up -d
#+END_SRC

The Ansible role =pass= covers this process for a brand new Ubuntu instance.

[fn:ssh-terraform] In reality, this is not probably necessary; AWS/Azure will likely configure
SSH keys for likely accounts in the instances, but that will require some Terraform configuration;
good task for later.
